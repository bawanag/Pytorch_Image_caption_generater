{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"COMP5623_CW2_TaoYuanjian.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e07561efe1034ab9aef36980edd0484e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d84c24fe222d4f73a7b33c6283f57b13","IPY_MODEL_187e427538bb4a968ecf43c4e5685863"],"layout":"IPY_MODEL_528f3d2389f348daa5092ce40364c732"}},"d84c24fe222d4f73a7b33c6283f57b13":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_0d3945999be34381b2b0c5f0ce1622da","max":241530880,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd7868ac5ea64dc5ad7111813d9d330d","value":241530880}},"187e427538bb4a968ecf43c4e5685863":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08ee45be88c241f9bebb05e7c758f0ab","placeholder":"​","style":"IPY_MODEL_65f4b12627f84377a08cf4c528648bd5","value":" 230M/230M [00:02&lt;00:00, 81.7MB/s]"}},"528f3d2389f348daa5092ce40364c732":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d3945999be34381b2b0c5f0ce1622da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd7868ac5ea64dc5ad7111813d9d330d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"08ee45be88c241f9bebb05e7c758f0ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65f4b12627f84377a08cf4c528648bd5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{},"source":["## import and path define"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YJI8jvJJty1h","executionInfo":{"elapsed":24355,"status":"ok","timestamp":1611894846033,"user":{"displayName":"bawanag ts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-86lNzD3ACeJ6O07Fw69-1x2vdN-X8vabfegW=s64","userId":"13666780178882475588"},"user_tz":-480},"outputId":"cc08c622-c4df-46ba-d9a7-43fc0670d0fb"},"source":["from torchvision import transforms\n","import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","from torch.nn.utils.rnn import pack_padded_sequence\n","import numpy as np\n","from torch.utils.data import Dataset\n","from nltk.translate.bleu_score import sentence_bleu\n","import matplotlib.pyplot as plt\n","import random\n","import datetime\n","import os\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"iXpWOFqFOXcc"},"source":["# Mounted Drive if using Colab; otherwise, your local path\n","root = os.path.abspath(os.path.dirname(os.getcwd())) + \"\\\\Flickr8k\\\\\"\n","model_path = root + \"models\\\\\"\n","vocab_file = model_path + \"vocab.object.pyobj\"\n","caption_dir = root + \"caption\\\\\"\n","image_dir = root + \"Flicker8k_Dataset\\\\\"\n","numpy_loss_dir = root\n","token_file = \"Flickr8k.token.txt\""],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D86cJx2yv81K"},"source":["## Text prepare"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oksUJjLPwApA","executionInfo":{"elapsed":27084,"status":"ok","timestamp":1611894848778,"user":{"displayName":"bawanag ts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-86lNzD3ACeJ6O07Fw69-1x2vdN-X8vabfegW=s64","userId":"13666780178882475588"},"user_tz":-480},"outputId":"9a11d702-5f72-405b-8188-3e74f200f840"},"source":["def read_lines(filepath):\n","    \"\"\" Open the ground truth captions into memory, line by line. \"\"\"\n","    file = open(filepath, 'r')\n","    lines = []\n","\n","    while True: \n","        # Get next line from file until there's no more\n","        line = file.readline() \n","        if not line: \n","            break\n","        lines.append(line.strip())\n","    file.close() \n","    return lines\n","lines = read_lines(caption_dir + token_file)\n","print(lines[:5])\n","class Vocabulary(object):\n","    \"\"\"Simple vocabulary wrapper which maps every unique word to an integer ID. \"\"\"\n","    def __init__(self):\n","        # Intially, set both the IDs and words to empty dictionaries.\n","        self.word2idx = {}\n","        self.idx2word = {}\n","        self.idx = 0\n","\n","    def add_word(self, word):\n","        # If the word does not already exist in the dictionary, add it\n","        if not word in self.word2idx:\n","            self.word2idx[word] = self.idx\n","            self.idx2word[self.idx] = word\n","            # Increment the ID for the next word\n","            self.idx += 1\n","    def delete_word(self, word):\n","        if word in self.word2idx:\n","            current_idx = self.word2idx[word]\n","            self.word2idx.pop(word)\n","            self.idx2word.pop(current_idx)\n","            # Increment the ID for the next word\n","    def translate_idx(self, idx):\n","        return self.idx2word[idx] # transfer the index to real word \n","    def __call__(self, word):\n","        # If we try to access a word in the dictionary which does not exist, return the <unk> id\n","        if not word in self.word2idx:\n","            return self.word2idx['<unk>']\n","        return self.word2idx[word]\n","\n","    def __len__(self):\n","        return len(self.word2idx)\n","vocab = Vocabulary()\n","vocabtemp = Vocabulary()\n","word_counter = np.zeros(8840)\n","for mix_sentence in lines:\n","  sentence = mix_sentence.split('#')[1]\n","  sentence = sentence.split('\\t')\n","  words = sentence[1].split(' ')\n","  words.pop(len(words)-1)\n","  if (\".\" in words):\n","    words.remove(\".\")\n","  if (\",\" in words):\n","    words.remove(\",\")\n","  for word in words:\n","    word = word.lower()\n","    vocabtemp.add_word(word)\n","    indx = vocabtemp(word)\n","    word_counter[indx] +=1\n","print(len(word_counter))\n","print(len(vocabtemp))\n","print(word_counter[1000:1100])\n","vocab.add_word('<pad>')\n","vocab.add_word('<start>')\n","vocab.add_word('<end>')\n","vocab.add_word('<unk>')\n","for l in range(len(vocabtemp)):\n","  if word_counter[l]>3:\n","    vocab.add_word(vocabtemp.translate_idx(l))\n","print(len(vocab))\n","\n","import pickle\n","f = open(vocab_file, 'wb')\n","pickle.dump(vocab, f)\n","f.close()"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["['1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of stairs in an entry way .', '1000268201_693b08cb0e.jpg#1\\tA girl going into a wooden building .', '1000268201_693b08cb0e.jpg#2\\tA little girl climbing into a wooden playhouse .', '1000268201_693b08cb0e.jpg#3\\tA little girl climbing the stairs to her playhouse .', '1000268201_693b08cb0e.jpg#4\\tA little girl in a pink dress going into a wooden cabin .']\n","8840\n","8840\n","[  5.  10.  38. 291. 121.   3.  26.   7.  13.  15. 103.  93.   7.  20.\n","  65.  25.   1.   2.   3.  38.  20.   4.  57.   7.   1.   6.   2.   5.\n","   1.   4.   2.  38.  67.   5.  23.   5.  11.  37.   7.  16.  30.  85.\n","  29. 153. 118.   4.  10. 315.   1.   1.  17.  88.  16.  16. 130.   2.\n","   2.  18.   6.  49.  33.  59.  57.  26.  62.  63.  42.  34.  26.  85.\n","   5.  67.   1.   2.   1.   3.   2.   5. 104.  25. 165.  36.  29.   1.\n","   7.  47.   4.  15.   9.  22.  14.  12.  19. 131.  30.   9.   9.  21.\n","   7.   1.]\n","3411\n"]}]},{"cell_type":"markdown","metadata":{"id":"FB30f4wYwSvg"},"source":["## Dataset and loaders for training\r\n","\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TYQz4T3mwA2o","executionInfo":{"elapsed":27518,"status":"ok","timestamp":1611894849220,"user":{"displayName":"bawanag ts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-86lNzD3ACeJ6O07Fw69-1x2vdN-X8vabfegW=s64","userId":"13666780178882475588"},"user_tz":-480},"outputId":"e5f46913-bb72-40e7-805b-9afd9a5a1a84"},"source":["def get_meta(lines):\n","    \"\"\" Fetches the meta data for all the images and assigns labels.\n","    \"\"\"\n","    image_ids, cleaned_captions = [], []\n","    for line in lines:\n","      mix_line0 = line.split('#')\n","      mix_line1 = mix_line0[0].split('.jpg')\n","      mix_sentence = mix_line0[1].split('\\t')\n","      image_ids.append(mix_line1[0])\n","      cleaned_captions.append(mix_sentence[1].lower())\n","                \n","    return image_ids, cleaned_captions\n","import pandas as pd\n","image_ids, cleaned_captions = get_meta(lines)\n","\n","data = {\n","    'image_id': image_ids,\n","    'path': [image_dir + image_id + \".jpg\" for image_id in image_ids],\n","    'caption': cleaned_captions\n","}\n","\n","data_df = pd.DataFrame(data, columns=['image_id', 'path', 'caption'])\n","print(data_df.head(n=5))"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["                image_id                                               path  \\\n0  1000268201_693b08cb0e  d:\\jianguoyun_syn\\学习\\artificial intelligence\\a...   \n1  1000268201_693b08cb0e  d:\\jianguoyun_syn\\学习\\artificial intelligence\\a...   \n2  1000268201_693b08cb0e  d:\\jianguoyun_syn\\学习\\artificial intelligence\\a...   \n3  1000268201_693b08cb0e  d:\\jianguoyun_syn\\学习\\artificial intelligence\\a...   \n4  1000268201_693b08cb0e  d:\\jianguoyun_syn\\学习\\artificial intelligence\\a...   \n\n                                             caption  \n0  a child in a pink dress is climbing up a set o...  \n1              a girl going into a wooden building .  \n2   a little girl climbing into a wooden playhouse .  \n3  a little girl climbing the stairs to her playh...  \n4  a little girl in a pink dress going into a woo...  \n"]}]},{"cell_type":"code","metadata":{"id":"wqf2_F6YwakD"},"source":["from PIL import Image\n","import cv2\n","from nltk import tokenize\n","class Flickr8k(Dataset):\n","    \"\"\" Flickr8k custom dataset compatible with torch.utils.data.DataLoader. \"\"\"\n","    \n","    def __init__(self, df, vocab, transform=None):\n","        \"\"\" Set the path for images, captions and vocabulary wrapper.\n","        \n","        Args:\n","            df: df containing image paths and captions.\n","            vocab: vocabulary wrapper.\n","            transform: image transformer.\n","        \"\"\"\n","        self.df = df\n","        self.vocab = vocab\n","        self.transform = transform\n","\n","    def __getitem__(self, index):\n","        \"\"\" Returns one data pair (image and caption). \"\"\"\n","\n","        vocab = self.vocab\n","\n","        caption = self.df['caption'][index].lower()\n","        img_id = self.df['image_id'][index]\n","        path = self.df['path'][index]\n","\n","        image = Image.open(open(path, 'rb'))\n","\n","        if self.transform is not None:\n","            image = self.transform(image)\n","\n","        # Convert caption (string) to word ids.\n","        tokens = caption.split()\n","        caption = []\n","        # Build the Tensor version of the caption, with token words\n","        caption.append(vocab('<start>'))\n","        for token in tokens:\n","          indextemp = vocab(token)\n","          if(indextemp != 3):\n","            caption.extend([indextemp])\n","        caption.append(vocab('<end>'))\n","        target = torch.Tensor(caption)\n","        return image, target\n","\n","    def __len__(self):\n","        return len(self.df)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5YmKr9ewkqO"},"source":["def caption_collate_fn(data):\n","    \"\"\" Creates mini-batch tensors from the list of tuples (image, caption).\n","    Args:\n","        data: list of tuple (image, caption). \n","            - image: torch tensor of shape (3, 256, 256).\n","            - caption: torch tensor of shape (?); variable length.\n","    Returns:\n","        images: torch tensor of shape (batch_size, 3, 256, 256).\n","        targets: torch tensor of shape (batch_size, padded_length).\n","        lengths: list; valid length for each padded caption.\n","    \"\"\"\n","    # Sort a data list by caption length from longest to shortest.\n","    data.sort(key=lambda x: len(x[1]), reverse=True)# labda is the function to suffle the data\n","    images, captions = zip(*data)\n","\n","    # Merge images (from tuple of 3D tensor to 4D tensor).\n","    images = torch.stack(images, 0)\n","\n","    # Merge captions (from tuple of 1D tensor to 2D tensor).\n","    lengths = [len(cap) for cap in captions]\n","    targets = torch.zeros(len(captions), max(lengths)).long()\n","    for i, cap in enumerate(captions):\n","        end = lengths[i]\n","        targets[i, :end] = cap[:end]        \n","    return images, targets, lengths\n","# Crop size matches the input dimensions expected by the pre-trained ResNet\n","data_transform = transforms.Compose([ \n","    transforms.Resize(224),\n","    transforms.CenterCrop(224),  # Why do we choose 224 x 224?\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.485, 0.456, 0.406),   # Using ImageNet norms\n","                         (0.229, 0.224, 0.225))])"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i_lIduCbfDEE"},"source":["## DataLoader"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KkNrIRbXGLFG","executionInfo":{"elapsed":27503,"status":"ok","timestamp":1611894849222,"user":{"displayName":"bawanag ts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-86lNzD3ACeJ6O07Fw69-1x2vdN-X8vabfegW=s64","userId":"13666780178882475588"},"user_tz":-480},"outputId":"fbd2187b-9279-4956-c0e5-3b1dc837da83"},"source":["unit_size = 5\n","\n","train_split = 0.90 # Defines the ratio of train/test data.\n","\n","test_split = 1 - train_split\n","train_size = unit_size * round(len(data_df)*train_split / unit_size)\n","\n","test_size = unit_size * round(len(data_df)*test_split / unit_size)\n","\n","dataset_train = Flickr8k(\n","    df=data_df[:].reset_index(drop=True),\n","    vocab=vocab,\n","    transform=data_transform,\n",")\n","\n","dataset_test = Flickr8k(\n","    df=data_df[train_size:].reset_index(drop=True),\n","    vocab=vocab,\n","    transform=data_transform,\n",")\n","train_loader = torch.utils.data.DataLoader(\n","  dataset_train,\n","  batch_size=256, \n","  shuffle=True,\n","  num_workers=0,\n","  collate_fn=caption_collate_fn\n",")\n","test_loader = torch.utils.data.DataLoader(\n","  dataset_test,\n","  batch_size=5, \n","  shuffle=False,\n","  num_workers=0,\n","  collate_fn=caption_collate_fn\n",")\n","print(train_loader.__len__())\n","print(test_loader.__len__())\n"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["270\n809\n"]}]},{"cell_type":"markdown","metadata":{"id":"oXlf8lt5TF0N"},"source":["## Encoder and decoder models"]},{"cell_type":"code","metadata":{"id":"ls8lyXA2GTC0"},"source":["\n","\n","class EncoderCNN(nn.Module):\n","    def __init__(self, embed_size):\n","        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n","        super(EncoderCNN, self).__init__()\n","\n","        resnet = models.resnet152(pretrained=True) # Pre-trained on ImageNet by default\n","        layers = list(resnet.children())[:-1]      # Keep all layers except the last one\n","        # Unpack the layers and create a new Sequential\n","        self.resnet = nn.Sequential(*layers)\n","        \n","        # We want a specific output size, which is the size of our embedding, so\n","        # we feed our extracted features from the last fc layer (dimensions 1 x 1000)\n","        # into a Linear layer to resize\n","        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n","        \n","        # Batch normalisation helps to speed up training\n","        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n","        self.ft = nn.Flatten() \n","\n","\n","    def forward(self, out):\n","      with torch.no_grad():\n","        out = self.resnet(out)\n","      out = self.ft(out)\n","      out =  self.linear(out)\n","      out = self.bn(out)\n","\n","      return out\n","\n","\n","class DecoderLTSM(nn.Module):\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n","        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n","        super(DecoderLTSM, self).__init__()\n","        \n","        self.embed = nn.Embedding(vocab_size, embed_size)\n","\n","        self.lstm = nn.LSTM(embed_size, hidden_size,batch_first=True)\n","        \n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","        self.max_seq_length = max_seq_length\n","        \n","    def forward(self, features, captions, lengths):\n","        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n","        embeddings = self.embed(captions)\n","        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n","        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n","        hiddens, _ = self.lstm(packed) # Replace with self.rnn when using RNN\n","        outputs = self.linear(hiddens[0])\n","        return outputs\n","    \n","    def sample(self, features, states=None):\n","        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n","        sampled_ids = []\n","        inputs = features.unsqueeze(1)\n","        for i in range(self.max_seq_length):\n","            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n","            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n","            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n","            sampled_ids.append(predicted)\n","            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n","            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n","        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n","        return sampled_ids\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dhecFOMRUgpe"},"source":["Set training parameters."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Fd2-IX2Uer3","executionInfo":{"elapsed":27942,"status":"ok","timestamp":1611894849674,"user":{"displayName":"bawanag ts","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg-86lNzD3ACeJ6O07Fw69-1x2vdN-X8vabfegW=s64","userId":"13666780178882475588"},"user_tz":-480},"outputId":"63440e55-9e86-44fc-d791-60f173423125"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","embed_size = 256\n","hidden_size = 512\n","num_layers = 1\n","learning_rate = 0.001\n","num_epochs = 5\n","log_step = 10\n","save_step = 300\n","init_epochs = 0"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","metadata":{"id":"AlIwF6P8UgB4"},"source":["Initialize the models and set the learning parameters."]},{"cell_type":"markdown","metadata":{"id":"RUmSb2MHEZw3"},"source":["## Training the model"]},{"cell_type":"markdown","metadata":{"id":"uS4oN21vNKu7"},"source":["The loop to train the model. Feel free to put this in a function if you prefer."]},{"cell_type":"code","metadata":{"id":"iLfdR4TCxUBW"},"source":["# Train the models\n","def train_model(encoder,decoder,criterion,optimizer,model_type):\n","  total_step = len(train_loader)\n","  loss_summarize = []\n","  for epoch in range(init_epochs, num_epochs):\n","    each_epoch_loss = []\n","    starttime = endtime = datetime.datetime.now()\n","    for i, (images, captions, lengths) in enumerate(train_loader):\n","      # Set mini-batch dataset\n","      images = images.to(device)\n","      captions = captions.to(device)\n","      # Packed as well as we'll compare to the decoder outputs\n","      targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n","\n","      # Forward, backward and optimize\n","      features = encoder(images)\n","      outputs = decoder(features, captions, lengths)\n","\n","      loss = criterion(outputs, targets)\n","      \n","      # Zero gradients for both networks\n","      decoder.zero_grad()\n","      encoder.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      # # Print log info\n","      if i % log_step == 0:\n","        endtime = datetime.datetime.now()\n","        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}， Time comsumption:{}'.format(epoch+1, num_epochs, i, total_step, loss.item(), endtime-starttime))\n","        starttime = endtime\n","        each_epoch_loss.append(loss.item())\n","    torch.save(decoder.state_dict(), model_path + model_type+'-decoder-epoch_{}.ckpt'.format(epoch+1))\n","    torch.save(encoder.state_dict(), model_path + model_type+'-encoder-epoch_{}.ckpt'.format(epoch+1))\n","    loss_summarize.append(each_epoch_loss)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"uxwDUlR2Uy7t","outputId":"4f5a64d3-2f15-403f-c28d-e26d6f36d0f8"},"source":["# Build the LSTM models\n","encoder = EncoderCNN(embed_size).to(device)\n","decoder = DecoderLTSM(embed_size, hidden_size, len(vocab), num_layers).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","\n","# Optimisation will be on the parameters of BOTH the enocder and decoder,\n","# but excluding the ResNet parameters, only the new added layers.\n","params = list(\n","    decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters()\n",")\n","\n","optimizer = torch.optim.Adam(params, lr=learning_rate)\n","model_type = \"lstm\"\n","train_model(encoder,decoder,criterion,optimizer,model_type)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/5], Step [0/270], Loss: 8.1340， Time comsumption:0:00:05.310206\n","Epoch [1/5], Step [10/270], Loss: 5.5332， Time comsumption:0:00:24.363346\n"]}]}]}